{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An investigation into use of SAEs as steering vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If in Colab, install deps. Otherwise, setup autoreload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "\n",
    "except ImportError:\n",
    "    # Local\n",
    "    IN_COLAB = False\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    ipython = IPython.get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pathlib\n",
    "\n",
    "import huggingface_hub\n",
    "from sae_lens import SAE\n",
    "from tqdm import tqdm, trange\n",
    "import torch as t\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "t.set_grad_enabled(False)\n",
    "\n",
    "if t.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"\\nDevice: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83871314b04a4b219638ff13afc35a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': 'cuda',\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LNPre',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "gpt2_small = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "pprint(gpt2_small.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GPT-2 J-B SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAEConfig(architecture='standard',\n",
      "          d_in=768,\n",
      "          d_sae=24576,\n",
      "          activation_fn_str='relu',\n",
      "          apply_b_dec_to_input=True,\n",
      "          finetuning_scaling_factor=False,\n",
      "          context_size=128,\n",
      "          model_name='gpt2-small',\n",
      "          hook_name='blocks.0.hook_resid_pre',\n",
      "          hook_layer=0,\n",
      "          hook_head_index=None,\n",
      "          prepend_bos=True,\n",
      "          dataset_path='Skylion007/openwebtext',\n",
      "          dataset_trust_remote_code=True,\n",
      "          normalize_activations='none',\n",
      "          dtype='torch.float32',\n",
      "          device='cuda',\n",
      "          sae_lens_training_version=None,\n",
      "          activation_fn_kwargs={},\n",
      "          neuronpedia_id='gpt2-small/0-res-jb')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "saes = [\n",
    "    SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "        device=DEVICE,\n",
    "    )[0]\n",
    "    for layer in trange(gpt2_small.cfg.n_layers)\n",
    "]\n",
    "\n",
    "pprint(saes[0].cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export SAE feature explanations for later search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_explanations_from_saes(saes, save_path):\n",
    "    try:\n",
    "        with open(save_path, \"r\") as f:\n",
    "            explanations = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "        explanations = []\n",
    "\n",
    "        for i, sae in enumerate(tqdm(saes)):\n",
    "            model, sae_id = sae.cfg.neuronpedia_id.split(\"/\")\n",
    "\n",
    "            querystring = {\"modelId\": model, \"saeId\": sae_id}\n",
    "            headers = {\"X-Api-Key\": os.getenv(\"NEURONPEDIA_TOKEN\")}\n",
    "\n",
    "            response = requests.get(url, headers=headers, params=querystring)\n",
    "            explanations += response.json()\n",
    "\n",
    "            with open(save_path, \"w\") as f:\n",
    "                json.dump(explanations, f, indent=2)\n",
    "\n",
    "    return explanations, save_path\n",
    "\n",
    "\n",
    "explanations_fpath = \"gpt2-small_res-jb_explanations.json\"\n",
    "explanations, _ = load_explanations_from_saes(saes, explanations_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all features whose explanations contain keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant features in layer 0: 3\n",
      "Number of relevant features in layer 1: 12\n",
      "Number of relevant features in layer 2: 7\n",
      "Number of relevant features in layer 3: 9\n",
      "Number of relevant features in layer 4: 8\n",
      "Number of relevant features in layer 5: 6\n",
      "Number of relevant features in layer 6: 5\n",
      "Number of relevant features in layer 7: 4\n",
      "Number of relevant features in layer 8: 5\n",
      "Number of relevant features in layer 9: 4\n",
      "Number of relevant features in layer 10: 3\n",
      "Number of relevant features in layer 11: 2\n",
      "Total relevant features: 68\n"
     ]
    }
   ],
   "source": [
    "def get_explanations_with_keywords_by_layer(explanations, keywords):\n",
    "    explanations_filtered = collections.defaultdict(list)\n",
    "\n",
    "    for explanation in explanations:\n",
    "        if any(keyword in explanation[\"description\"].upper() for keyword in keywords):\n",
    "            layer = int(explanation[\"layer\"].split(\"-\")[0])\n",
    "            explanations_filtered[layer].append(explanation)\n",
    "\n",
    "    return explanations_filtered\n",
    "\n",
    "\n",
    "keywords = [\"AUSTRALIA\"]\n",
    "\n",
    "explanations_filtered = get_explanations_with_keywords_by_layer(explanations, keywords)\n",
    "\n",
    "explanation_count = 0\n",
    "for layer in range(len(saes)):\n",
    "    explanation_count_in_layer = len(explanations_filtered[layer])\n",
    "    explanation_count += explanation_count_in_layer\n",
    "    print(f\"Number of relevant features in layer {layer}: {explanation_count_in_layer}\")\n",
    "print(f\"Total relevant features: {explanation_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find SAE feature indices that correlate with intended steering direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([[50256,    50,  5173,  1681, 26049,  2097]], device='cuda:0')\n",
      "Token strings: ['<|endoftext|>', 'S', 'yd', 'ney', ' Opera', ' House']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 1332.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered activations:\n",
      "{0: {'idx': [], 'val': []},\n",
      " 1: {'idx': [14665], 'val': [5.82246732711792]},\n",
      " 2: {'idx': [20864], 'val': [7.139648914337158]},\n",
      " 3: {'idx': [19448], 'val': [2.3950774669647217]},\n",
      " 4: {'idx': [19972], 'val': [4.3374762535095215]},\n",
      " 5: {'idx': [], 'val': []},\n",
      " 6: {'idx': [], 'val': []},\n",
      " 7: {'idx': [], 'val': []},\n",
      " 8: {'idx': [], 'val': []},\n",
      " 9: {'idx': [3036], 'val': [3.887145757675171]},\n",
      " 10: {'idx': [10541, 24077], 'val': [6.6507368087768555, 1.4402518272399902]},\n",
      " 11: {'idx': [], 'val': []}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_model_and_get_filtered_activations(\n",
    "    model, prompt, explanations_filtered, activation_threshold, quiet=True\n",
    "):\n",
    "    _, cache = model.run_with_cache(prompt, prepend_bos=True)\n",
    "\n",
    "    if not quiet:\n",
    "        tokens = model.to_tokens(prompt)\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token strings: {model.to_str_tokens(tokens)}\")\n",
    "\n",
    "    saes_out = {}\n",
    "    acts_filtered = {}\n",
    "\n",
    "    for layer, sae in enumerate(tqdm(saes)):\n",
    "        if explanations_filtered[layer] == []:\n",
    "            continue\n",
    "\n",
    "        explanations_filtered_idx = t.tensor(\n",
    "            [int(explanation[\"index\"]) for explanation in explanations_filtered[layer]],\n",
    "            device=DEVICE,\n",
    "        )\n",
    "\n",
    "        feature_acts = sae.encode(\n",
    "            cache[sae.cfg.hook_name]\n",
    "        )  # shape (batch, sequence, features)\n",
    "\n",
    "        saes_out[layer] = sae.decode(sv_feature_acts)\n",
    "\n",
    "        feature_acts_vals_sorted, idx = feature_acts[\n",
    "            :, :, explanations_filtered_idx\n",
    "        ].sort(descending=True, dim=-1)\n",
    "\n",
    "        feature_acts_idx_sorted = explanations_filtered_idx[idx]\n",
    "\n",
    "        mask = feature_acts_vals_sorted >= activation_threshold\n",
    "\n",
    "        acts_filtered[layer] = {\n",
    "            \"val\": feature_acts_vals_sorted[mask].tolist(),\n",
    "            \"idx\": feature_acts_idx_sorted[mask].tolist(),\n",
    "        }\n",
    "\n",
    "    return (\n",
    "        cache,\n",
    "        acts_filtered,\n",
    "        saes_out,\n",
    "    )\n",
    "\n",
    "\n",
    "def print_relevant_features(feature_act_vals_sorted, feature_act_idx_sorted, model):\n",
    "    for layer in range(len(saes)):\n",
    "        if explanations_filtered[layer] == []:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nSorted activations for layer {layer}\")\n",
    "        pprint(feature_act_vals_sorted[layer])\n",
    "        pprint(feature_act_idx_sorted[layer])\n",
    "\n",
    "\n",
    "sv_prompt = \"Sydney Opera House\"\n",
    "activation_threshold = 1\n",
    "\n",
    "cache, acts_filtered, saes_out = run_model_and_get_filtered_activations(\n",
    "    gpt2_small, sv_prompt, explanations_filtered, activation_threshold, quiet=False\n",
    ")\n",
    "\n",
    "# print_relevant_features(act_vals, act_idx, gpt2_small)\n",
    "\n",
    "print(\"\\nFiltered activations:\")\n",
    "pprint(acts_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hooked_generate(prompt_batch, fwd_hooks=[], max_new_tokens=20, seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        t.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_generate_single_layer(\n",
    "    prompt,\n",
    "    layer,\n",
    "    model,\n",
    "    coeff,\n",
    "    saes,\n",
    "    saes_out,\n",
    "    sampling_kwargs,\n",
    "    n=3,\n",
    "    steering_on=True,\n",
    "    max_new_tokens=20,\n",
    "):\n",
    "    steering_vector = saes[steering_layer].W_dec[20864]\n",
    "    sae_out = saes_out[steering_layer]\n",
    "\n",
    "    def steering_hook(resid_pre, hook):\n",
    "        if resid_pre.shape[1] == 1:\n",
    "            return\n",
    "\n",
    "        position = sae_out.shape[1]\n",
    "        if steering_on:\n",
    "            # using our steering vector and applying the coefficient\n",
    "            resid_pre[:, : position - 1, :] += coeff * steering_vector\n",
    "\n",
    "    model.reset_hooks()\n",
    "    editing_hooks = [(f\"blocks.{layer}.hook_resid_post\", steering_hook)]\n",
    "    res = hooked_generate(\n",
    "        [prompt] * n,\n",
    "        editing_hooks,\n",
    "        max_new_tokens,\n",
    "        seed=None,\n",
    "        **sampling_kwargs,\n",
    "    )\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    # res_str = model.to_string(res[:, 1:])\n",
    "    # print((\"\\n\" + \"-\" * 80 + \"\\n\").join(res_str))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933b31bdb12945aca0bfa0f3d8765b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0a7edaf36040bba6b3a73d87f180a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>The White House is located in a country called Australia, and '\n",
      " 'the president has been visiting the country for more than a year.\\n'\n",
      " '\\n'\n",
      " 'But he',\n",
      " '<|endoftext|>The White House is located in a country called the United '\n",
      " \"States of America, and it's not just about being a place where people can \"\n",
      " 'come and',\n",
      " '<|endoftext|>The White House is located in a country called Australia, and '\n",
      " 'the United States has been a major player in the world of sports since it '\n",
      " 'was founded']\n",
      "['<|endoftext|>The White House is located in a country called the United '\n",
      " \"States of America, and it's not just that. It's also that the United States\",\n",
      " '<|endoftext|>The White House is located in a country called the United '\n",
      " \"States of America, and it's not just the president who has been on the \"\n",
      " 'receiving end',\n",
      " '<|endoftext|>The White House is located in a country called the United '\n",
      " \"States of America, and it's not just that. It's also that the United States\"]\n"
     ]
    }
   ],
   "source": [
    "steering_layer = 2\n",
    "\n",
    "example_prompt = \"The White House is located in a country called\"\n",
    "coeff = 100\n",
    "\n",
    "steering_kwargs = {\"model\": gpt2_small, \"saes\": saes, \"saes_out\": saes_out}\n",
    "sampling_kwargs = {\"temperature\": 1.0, \"top_p\": 0.1, \"freq_penalty\": 1.0}\n",
    "\n",
    "res_on = run_generate(\n",
    "    example_prompt,\n",
    "    layer=steering_layer,\n",
    "    steering_on=True,\n",
    "    coeff=coeff,\n",
    "    sampling_kwargs=sampling_kwargs,\n",
    "    **steering_kwargs,\n",
    ")\n",
    "res_off = run_generate(\n",
    "    example_prompt,\n",
    "    layer=steering_layer,\n",
    "    steering_on=False,\n",
    "    coeff=coeff,\n",
    "    sampling_kwargs=sampling_kwargs,\n",
    "    **steering_kwargs,\n",
    ")\n",
    "\n",
    "pprint(gpt2_small.to_string(res_on))\n",
    "pprint(gpt2_small.to_string(res_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to Gemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edc7860bcd04ebabc2a933bce1cf341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gemma_2_2b \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2-2b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m pprint(gemma_2_2b\u001b[38;5;241m.\u001b[39mcfg)\n",
      "File \u001b[1;32mc:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformer_lens\\HookedTransformer.py:1301\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[1;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001b[0m\n\u001b[0;32m   1296\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m loading\u001b[38;5;241m.\u001b[39mget_pretrained_state_dict(\n\u001b[0;32m   1297\u001b[0m     official_model_name, cfg, hf_model, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pretrained_kwargs\n\u001b[0;32m   1298\u001b[0m )\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;66;03m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m-> 1301\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmove_to_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_padding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_padding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m model\u001b[38;5;241m.\u001b[39mload_and_process_state_dict(\n\u001b[0;32m   1309\u001b[0m     state_dict,\n\u001b[0;32m   1310\u001b[0m     fold_ln\u001b[38;5;241m=\u001b[39mfold_ln,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     refactor_factored_attn_matrices\u001b[38;5;241m=\u001b[39mrefactor_factored_attn_matrices,\n\u001b[0;32m   1315\u001b[0m )\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m move_to_device:\n",
      "File \u001b[1;32mc:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformer_lens\\HookedTransformer.py:177\u001b[0m, in \u001b[0;36mHookedTransformer.__init__\u001b[1;34m(self, cfg, tokenizer, move_to_device, default_padding_side)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_tokens:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_tokens \u001b[38;5;241m=\u001b[39m HookPoint()  \u001b[38;5;66;03m# [batch, pos]\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 177\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblock_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnormalization_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final \u001b[38;5;241m=\u001b[39m RMSNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n",
      "File \u001b[1;32mc:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformer_lens\\HookedTransformer.py:177\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_tokens:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_tokens \u001b[38;5;241m=\u001b[39m HookPoint()  \u001b[38;5;66;03m# [batch, pos]\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m--> 177\u001b[0m     [\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m block_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_layers)]\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnormalization_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final \u001b[38;5;241m=\u001b[39m RMSNorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n",
      "File \u001b[1;32mc:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformer_lens\\components\\transformer_block.py:83\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, cfg, block_index)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_types must be set when using local attention\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     attn_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_types[block_index]\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m MLPFactory\u001b[38;5;241m.\u001b[39mcreate_mlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n",
      "File \u001b[1;32mc:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformer_lens\\components\\grouped_query_attention.py:32\u001b[0m, in \u001b[0;36mGroupedQueryAttention.__init__\u001b[1;34m(self, cfg, attn_type, layer_id)\u001b[0m\n\u001b[0;32m     30\u001b[0m cfg \u001b[38;5;241m=\u001b[39m HookedTransformerConfig\u001b[38;5;241m.\u001b[39munwrap(cfg)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mn_key_value_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat_kv_heads \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m cfg\u001b[38;5;241m.\u001b[39mn_key_value_heads\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_W_K \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[0;32m     36\u001b[0m         cfg\u001b[38;5;241m.\u001b[39mn_key_value_heads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformer_lens\\components\\abstract_attention.py:81\u001b[0m, in \u001b[0;36mAbstractAttention.__init__\u001b[1;34m(self, cfg, attn_type, layer_id)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_type \u001b[38;5;241m=\u001b[39m attn_type\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Create a max_ctx x max_ctx mask, with True iff that query position\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# can attend to that key position (query is first axis, key is second axis)\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbool())\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# For global attention, this is a lower triangular matrix - key <= query\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m, causal_mask)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gemma_2_2b = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2-2b\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "pprint(gemma_2_2b.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gemma 2 SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 16\n",
    "\n",
    "saes = []\n",
    "for i in trange(gemma_2_2b.cfg.n_layers):\n",
    "    print(f\"Downloading canonical SAE for layer {i} and width {width}k\")\n",
    "    saes.append(\n",
    "        SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=f\"layer_{i}/width_{width}k/canonical\",\n",
    "            device=DEVICE,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "pprint(saes[0].cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sae in tqdm(saes):\n",
    "    fpath = pathlib.Path(\n",
    "        f\"./gemma-scope-2b-pt-res-canonical/layer_{sae.cfg.hook_layer}__width_{str(sae.cfg.d_sae)[:-3]}k\"\n",
    "    )\n",
    "    fpath.mkdir(parents=True, exist_ok=True)\n",
    "    sae.save_model(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all features whose explanations contain keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATIONS_GEMMA_FPATH = \"gemma-scope-2b-pt-res-canonical-w16k_explanations.json\"\n",
    "\n",
    "try:\n",
    "    with open(EXPLANATIONS_GEMMA_FPATH, \"r\") as f:\n",
    "        explanations = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "    explanations = []\n",
    "\n",
    "    for i in trange(len(saes)):\n",
    "        sae = saes[i]\n",
    "        model, sae_id = sae.cfg.neuronpedia_id.split(\"/\")\n",
    "\n",
    "        querystring = {\"modelId\": model, \"saeId\": sae_id}\n",
    "\n",
    "        headers = {\"X-Api-Key\": os.getenv(\"NEURONPEDIA_TOKEN\")}\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "        explanations += response.json()\n",
    "\n",
    "    with open(EXPLANATIONS_GEMMA_FPATH, \"w\") as f:\n",
    "        json.dump(explanations, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = [\"POTTER\"]\n",
    "\n",
    "explanations_filtered = [[] for i in range(len(saes))]\n",
    "explanation_count = 0\n",
    "\n",
    "for explanation in explanations:\n",
    "    if any(keyword in explanation[\"description\"].upper() for keyword in KEYWORDS):\n",
    "        layer = int(explanation[\"layer\"].split(\"-\")[0])\n",
    "        explanations_filtered[layer].append(explanation)\n",
    "        explanation_count += 1\n",
    "\n",
    "for i in range(len(saes)):\n",
    "    print(f\"Number of relevant features in layer {i}: {len(explanations_filtered[i])}\")\n",
    "\n",
    "print(f\"Total relevant features: {explanation_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_prompt = \"Albus Dumbledore\"\n",
    "sv_logits, cache = gemma_2_2b.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens = gemma_2_2b.to_tokens(sv_prompt)\n",
    "str_tokens = gemma_2_2b.to_str_tokens(tokens)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token strings: {str_tokens}\")\n",
    "\n",
    "k = 6\n",
    "act_threshold_relative = 0.005\n",
    "\n",
    "saes_out = []\n",
    "sv_feature_acts_vals_sorted_all_layers = []\n",
    "sv_feature_acts_idx_sorted_all_layers = []\n",
    "\n",
    "for i, sae in enumerate(saes):\n",
    "    explanations_filtered_idx = t.tensor(\n",
    "        [int(explanation[\"index\"]) for explanation in explanations_filtered[i]],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    if explanations_filtered_idx.numel() == 0:\n",
    "        continue\n",
    "\n",
    "    sv_feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    saes_out.append(sae.decode(sv_feature_acts))\n",
    "\n",
    "    act_max = sv_feature_acts.max()\n",
    "    act_threshold = act_threshold_relative * act_max\n",
    "\n",
    "    sv_feature_acts_filtered = sv_feature_acts[:, :, explanations_filtered_idx]\n",
    "\n",
    "    if (\n",
    "        sv_feature_acts_filtered.sum() * sv_feature_acts_filtered.numel()\n",
    "        < act_threshold\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    sv_feature_acts_vals_sorted, sv_feature_acts_idx_sorted = (\n",
    "        sv_feature_acts_filtered.sort(descending=True, dim=-1)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSorted activations for layer {i}\")\n",
    "    print(f\"\\tMax activation for layer {i}: {act_max}\")\n",
    "    for token_idx, str_token in enumerate(str_tokens):\n",
    "        vals_for_token = sv_feature_acts_vals_sorted[0, token_idx]\n",
    "        idx_for_token = explanations_filtered_idx[sv_feature_acts_idx_sorted][\n",
    "            0, token_idx\n",
    "        ]\n",
    "        mask = vals_for_token >= act_threshold\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        print(f\"\\tToken {token_idx}: '{str_token}'\")\n",
    "        print(f\"\\t\\tRelevant feature activations: {vals_for_token[mask].tolist()}\")\n",
    "        print(f\"\\t\\tRelevant feature indices: {idx_for_token[mask].tolist()}\")\n",
    "\n",
    "# from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# get_neuronpedia_quick_list(\n",
    "#     sae=sae, features=sv_feature_acts_idx[:, :, :].flatten().tolist()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEERING_LAYER = 8\n",
    "steering_vector = saes[STEERING_LAYER].W_dec[3012]\n",
    "\n",
    "example_prompt = \"My favourite protagonist in any fantasy novel series is named\"\n",
    "coeff = 300\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "\n",
    "model = gemma_2_2b\n",
    "\n",
    "sae_out = saes_out[STEERING_LAYER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "\n",
    "    position = sae_out.shape[1]\n",
    "    if steering_on:\n",
    "        # using our steering vector and applying the coefficient\n",
    "        resid_pre[:, : position - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        t.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(example_prompt):\n",
    "    model.reset_hooks()\n",
    "    editing_hooks = [(f\"blocks.{STEERING_LAYER}.hook_resid_post\", steering_hook)]\n",
    "    res = hooked_generate(\n",
    "        [example_prompt] * 3, editing_hooks, seed=None, **sampling_kwargs\n",
    "    )\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    res_str = model.to_string(res[:, 1:])\n",
    "    print((\"\\n\" + \"-\" * 80 + \"\\n\").join(res_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_on = True\n",
    "run_generate(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_on = False\n",
    "run_generate(example_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
