{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An investigation into use of SAEs as steering vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Google Colab\n",
    "    # import google.colab  # type: ignore\n",
    "    from google.colab import output\n",
    "\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "\n",
    "except ImportError:\n",
    "    # Local\n",
    "    COLAB = False\n",
    "    import IPython  # type: ignore\n",
    "\n",
    "    ipython = IPython.get_ipython()\n",
    "    assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "import webbrowser\n",
    "import http.server\n",
    "import requests\n",
    "import socketserver\n",
    "import threading\n",
    "\n",
    "PORT = 8000\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import torch as t\n",
    "from tqdm import tqdm, trange\n",
    "import plotly.express as px\n",
    "from pprint import pprint\n",
    "import pathlib\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# package import\n",
    "from functools import partial\n",
    "from jaxtyping import Int, Float\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import json\n",
    "\n",
    "# Device setup\n",
    "if t.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"\\nDevice: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc19ec8e9334e1590baac1bb09fe164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    \"\"\"\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    \"\"\"\n",
    "    if not (COLAB):\n",
    "        webbrowser.open(filename)\n",
    "    else:\n",
    "        global PORT\n",
    "\n",
    "        def serve(directory):\n",
    "            os.chdir(directory)\n",
    "\n",
    "            # Create a handler for serving files\n",
    "            handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "            # Create a socket server with the handler\n",
    "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
    "                print(f\"Serving files from {directory} on port {PORT}\")\n",
    "                httpd.serve_forever()\n",
    "\n",
    "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
    "        thread.start()\n",
    "\n",
    "        output.serve_kernel_port_as_iframe(\n",
    "            PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True\n",
    "        )\n",
    "\n",
    "        PORT += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get gpt2_small\n",
    "# gpt2_small = HookedTransformer.from_pretrained(\n",
    "#     \"gemma-2b\",\n",
    "#     device=DEVICE,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "pprint(gpt2_small.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GPT-2 SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# width = 16\n",
    "# f\"layer_{layer}/width_{width}k/canonical\"\n",
    "\n",
    "# get the SAE for this layer\n",
    "saes = [\n",
    "    SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "        device=DEVICE,\n",
    "    )[0]\n",
    "    for layer in range(12)\n",
    "]\n",
    "\n",
    "pprint(saes[0].cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export SAE feature explanations for later search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATIONS_FPATH = \"gpt2-small_res-jb_explanations.json\"\n",
    "\n",
    "try:\n",
    "    with open(EXPLANATIONS_FPATH, \"r\") as f:\n",
    "        explanations = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "    explanations = []\n",
    "\n",
    "    for i in trange(len(saes)):\n",
    "        sae = saes[i]\n",
    "        model, sae_id = sae.cfg.neuronpedia_id.split(\"/\")\n",
    "\n",
    "        querystring = {\"modelId\": model, \"saeId\": sae_id}\n",
    "\n",
    "        headers = {\"X-Api-Key\": os.getenv(\"NEURONPEDIA_TOKEN\")}\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "        explanations += response.json()\n",
    "\n",
    "        with open(EXPLANATIONS_FPATH, \"w\") as f:\n",
    "            json.dump(explanations, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all features whose explanations contain keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = [\"AUSTRALIA\"]\n",
    "\n",
    "explanations_filtered = [[] for i in range(12)]\n",
    "explanation_count = 0\n",
    "\n",
    "for explanation in explanations:\n",
    "    if any(keyword in explanation[\"description\"].upper() for keyword in KEYWORDS):\n",
    "        layer = int(explanation[\"layer\"].split(\"-\")[0])\n",
    "        explanations_filtered[layer].append(explanation)\n",
    "        explanation_count += 1\n",
    "\n",
    "for i in range(12):\n",
    "    print(f\"Number of relevant features in layer {i}: {len(explanations_filtered[i])}\")\n",
    "\n",
    "print(f\"Total relevant features: {explanation_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find SAE feature indices that correlate with intended steering direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_prompt = \"Australia\"\n",
    "sv_logits, cache = gpt2_small.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens = gpt2_small.to_tokens(sv_prompt)\n",
    "str_tokens = gpt2_small.to_str_tokens(tokens)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token strings: {str_tokens}\")\n",
    "\n",
    "k = 6\n",
    "act_threshold_relative = 0.005\n",
    "\n",
    "saes_out = []\n",
    "sv_feature_acts_vals_sorted_all_layers = []\n",
    "sv_feature_acts_idx_sorted_all_layers = []\n",
    "\n",
    "for i, sae in enumerate(saes):\n",
    "    explanations_filtered_idx = t.tensor(\n",
    "        [int(explanation[\"index\"]) for explanation in explanations_filtered[i]],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    sv_feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    saes_out.append(sae.decode(sv_feature_acts))\n",
    "\n",
    "    act_max = sv_feature_acts.max()\n",
    "    act_threshold = act_threshold_relative * act_max\n",
    "\n",
    "    sv_feature_acts_filtered = sv_feature_acts[:, :, explanations_filtered_idx]\n",
    "\n",
    "    if (\n",
    "        sv_feature_acts_filtered.sum() * sv_feature_acts_filtered.numel()\n",
    "        < act_threshold\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    sv_feature_acts_vals_sorted, sv_feature_acts_idx_sorted = (\n",
    "        sv_feature_acts_filtered.sort(descending=True, dim=-1)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSorted activations for layer {i}\")\n",
    "    print(f\"\\tMax activation for layer {i}: {act_max}\")\n",
    "    for token_idx, str_token in enumerate(str_tokens):\n",
    "        vals_for_token = sv_feature_acts_vals_sorted[0, token_idx]\n",
    "        idx_for_token = explanations_filtered_idx[sv_feature_acts_idx_sorted][\n",
    "            0, token_idx\n",
    "        ]\n",
    "        mask = vals_for_token >= act_threshold\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        print(f\"\\tToken {token_idx}: '{str_token}'\")\n",
    "        print(f\"\\t\\tRelevant feature activations: {vals_for_token[mask].tolist()}\")\n",
    "        print(f\"\\t\\tRelevant feature indices: {idx_for_token[mask].tolist()}\")\n",
    "\n",
    "# from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# get_neuronpedia_quick_list(\n",
    "#     sae=sae, features=sv_feature_acts_idx[:, :, :].flatten().tolist()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEERING_LAYER = 0\n",
    "steering_vector = sae.W_dec[15642]\n",
    "\n",
    "example_prompt = \"In which country would I find the White House?\"\n",
    "coeff = 3000\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)\n",
    "\n",
    "model = gpt2_small\n",
    "\n",
    "sae_out = saes_out[STEERING_LAYER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "\n",
    "    position = sae_out.shape[1]\n",
    "    if steering_on:\n",
    "        # using our steering vector and applying the coefficient\n",
    "        resid_pre[:, : position - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        t.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(example_prompt):\n",
    "    model.reset_hooks()\n",
    "    editing_hooks = [(f\"blocks.{layer}.hook_resid_post\", steering_hook)]\n",
    "    res = hooked_generate(\n",
    "        [example_prompt] * 3, editing_hooks, seed=None, **sampling_kwargs\n",
    "    )\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    res_str = model.to_string(res[:, 1:])\n",
    "    print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_on = True\n",
    "run_generate(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_on = False\n",
    "run_generate(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to Gemma 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ad488bf23e463a96744e870e5cc634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_pytorch_tanh',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 16.0,\n",
      " 'attn_scores_soft_cap': 50.0,\n",
      " 'attn_types': ['global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local',\n",
      "                'global',\n",
      "                'local'],\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 256,\n",
      " 'd_mlp': 9216,\n",
      " 'd_model': 2304,\n",
      " 'd_vocab': 256000,\n",
      " 'd_vocab_out': 256000,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': 'cuda',\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-06,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': True,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': True,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gemma-2-2b',\n",
      " 'n_ctx': 8192,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 8,\n",
      " 'n_key_value_heads': 4,\n",
      " 'n_layers': 26,\n",
      " 'n_params': 2146959360,\n",
      " 'normalization_type': 'RMSPre',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'Gemma2ForCausalLM',\n",
      " 'output_logits_soft_cap': 30.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000.0,\n",
      " 'rotary_dim': 256,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'google/gemma-2-2b',\n",
      " 'tokenizer_prepends_bos': True,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': True,\n",
      " 'use_normalization_before_and_after': True,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': 4096}\n"
     ]
    }
   ],
   "source": [
    "gemma_2_2b = HookedTransformer.from_pretrained(\n",
    "    \"gemma-2-2b\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "pprint(gemma_2_2b.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Gemma 2 SAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 0 and width 16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/26 [00:12<05:18, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 1 and width 16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/26 [00:13<02:14,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 2 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62045b452b3a4133893f0dec24811de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:  45%|####5     | 136M/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/26 [00:44<06:33, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 3 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d702735a0f43fd87004f8d45d7b5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 4/26 [01:23<09:26, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 4 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e82d7c5b4e4448a3224bac4aea0d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 5/26 [02:03<10:50, 30.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 5 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c5a73a834f48f08fcfa0e231e9d568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 6/26 [02:42<11:18, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 6 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf443f4e4504c2fa05783007650745d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 7/26 [03:17<10:45, 33.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 7 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dd84a925d3417c81ddfc589d466b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 8/26 [03:50<10:10, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 8 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06f63144efa4556ae11b4756d35e194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 9/26 [04:29<10:00, 35.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 9 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74590e07c0ce4589abb69b40236b0623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 10/26 [05:02<09:12, 34.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 10 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6118c2549134485b9d1993d3fe8ef17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 11/26 [05:37<08:42, 34.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 11 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c419ff4613e4262a62a4716a164fe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 12/26 [06:11<08:02, 34.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 12 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4a47782e544e7ea652ec0beed50fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 13/26 [06:44<07:23, 34.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 13 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2c5ce5f10f4f5fa1402a22c9004a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 14/26 [07:17<06:46, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 14 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3610a1aba73491380923086f2d5e8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 15/26 [07:51<06:10, 33.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 15 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f514bf3811234b65b7122e494152ceb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 16/26 [08:25<05:39, 33.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 16 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbe5233149547e0a86840153c460a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 17/26 [09:07<05:25, 36.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 17 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdadb3cff24477aa74dfdc699acec57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 18/26 [09:43<04:50, 36.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 18 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822daa165c9e4ac9b6a55f0fcd57affe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 19/26 [10:18<04:11, 35.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 19 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b3a89072b440b1826e75a2d802507e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 20/26 [10:59<03:43, 37.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 20 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce14112e0cd54900b88ea61e91e7aa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 21/26 [12:01<03:44, 44.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 21 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb32a85a725c40a1b6691f3b816c5f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 22/26 [12:54<03:09, 47.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 22 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8777f150354ebd9bd7cfea5c3024d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 23/26 [14:03<02:41, 53.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 23 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba64a5dd5d2e4db18a6bbe74a649ea55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 24/26 [14:59<01:48, 54.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 24 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01dabd26637460f9f215de48c0f297a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 25/26 [15:42<00:51, 51.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading canonical SAE for layer 25 and width 16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af09961345e49beb17dfccbc44221a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [16:23<00:00, 37.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAEConfig(architecture='jumprelu',\n",
      "          d_in=2304,\n",
      "          d_sae=16384,\n",
      "          activation_fn_str='relu',\n",
      "          apply_b_dec_to_input=False,\n",
      "          finetuning_scaling_factor=False,\n",
      "          context_size=1024,\n",
      "          model_name='gemma-2-2b',\n",
      "          hook_name='blocks.0.hook_resid_post',\n",
      "          hook_layer=0,\n",
      "          hook_head_index=None,\n",
      "          prepend_bos=True,\n",
      "          dataset_path='monology/pile-uncopyrighted',\n",
      "          dataset_trust_remote_code=True,\n",
      "          normalize_activations=None,\n",
      "          dtype='float32',\n",
      "          device='cuda',\n",
      "          sae_lens_training_version=None,\n",
      "          activation_fn_kwargs={},\n",
      "          neuronpedia_id='gemma-2-2b/0-gemmascope-res-16k')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "width = 16\n",
    "\n",
    "saes = []\n",
    "for i in trange(gemma_2_2b.cfg.n_layers):\n",
    "    print(f\"Downloading canonical SAE for layer {i} and width {width}k\")\n",
    "    saes.append(\n",
    "        SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=f\"layer_{i}/width_{width}k/canonical\",\n",
    "            device=DEVICE,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "pprint(saes[0].cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " 'W_dec',\n",
       " 'W_enc',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_enable_hook',\n",
       " '_enable_hook_with_name',\n",
       " '_enable_hooks_for_points',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'activation_fn',\n",
       " 'add_caching_hooks',\n",
       " 'add_hook',\n",
       " 'add_module',\n",
       " 'add_perma_hook',\n",
       " 'apply',\n",
       " 'apply_finetuning_scaling_factor',\n",
       " 'b_dec',\n",
       " 'b_enc',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'cache_all',\n",
       " 'cache_some',\n",
       " 'call_super_init',\n",
       " 'cfg',\n",
       " 'check_and_add_hook',\n",
       " 'check_hooks_to_add',\n",
       " 'children',\n",
       " 'clear_contexts',\n",
       " 'compile',\n",
       " 'context_level',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'd_head',\n",
       " 'decode',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'encode',\n",
       " 'encode_gated',\n",
       " 'encode_jumprelu',\n",
       " 'encode_standard',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'fold_W_dec_norm',\n",
       " 'fold_activation_norm_scaling_factor',\n",
       " 'forward',\n",
       " 'from_dict',\n",
       " 'from_pretrained',\n",
       " 'get_buffer',\n",
       " 'get_caching_hooks',\n",
       " 'get_extra_state',\n",
       " 'get_name',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'hook_dict',\n",
       " 'hook_points',\n",
       " 'hook_sae_acts_post',\n",
       " 'hook_sae_acts_pre',\n",
       " 'hook_sae_error',\n",
       " 'hook_sae_input',\n",
       " 'hook_sae_output',\n",
       " 'hook_sae_recons',\n",
       " 'hook_z_reshaping_mode',\n",
       " 'hooks',\n",
       " 'initialize_weights_basic',\n",
       " 'initialize_weights_gated',\n",
       " 'initialize_weights_jumprelu',\n",
       " 'ipu',\n",
       " 'is_caching',\n",
       " 'load_from_pretrained',\n",
       " 'load_state_dict',\n",
       " 'mod_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'remove_all_hook_fns',\n",
       " 'requires_grad_',\n",
       " 'reset_hooks',\n",
       " 'reshape_fn_in',\n",
       " 'reshape_fn_out',\n",
       " 'run_time_activation_norm_fn_in',\n",
       " 'run_time_activation_norm_fn_out',\n",
       " 'run_with_cache',\n",
       " 'run_with_hooks',\n",
       " 'save_model',\n",
       " 'set_extra_state',\n",
       " 'setup',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'threshold',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'turn_off_forward_pass_hook_z_reshaping',\n",
       " 'turn_on_forward_pass_hook_z_reshaping',\n",
       " 'type',\n",
       " 'use_error_term',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sae in saes:\n",
    "    fpath = Pathlib\"./\"\n",
    "    sae.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
